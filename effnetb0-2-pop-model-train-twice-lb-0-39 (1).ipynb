{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782},{"sourceId":7487364,"sourceType":"datasetVersion","datasetId":4359072},{"sourceId":7517324,"sourceType":"datasetVersion","datasetId":4378712},{"sourceId":7621177,"sourceType":"datasetVersion","datasetId":4439202},{"sourceId":158958765,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":270.012179,"end_time":"2024-01-14T22:56:02.916427","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-14T22:51:32.904248","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os #, gc\nimport time\nimport json\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\nimport tensorflow.keras.backend as K, gc\nimport tensorflow as tf\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\n\nprint('TensorFlow version =',tf.__version__)\n\n# USE MULTIPLE GPUS\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus)<=1: \n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')\n\n","metadata":{"papermill":{"duration":14.80928,"end_time":"2024-01-14T22:51:51.64702","exception":false,"start_time":"2024-01-14T22:51:36.83774","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T02:59:27.46585Z","iopub.execute_input":"2024-02-14T02:59:27.466143Z","iopub.status.idle":"2024-02-14T02:59:42.154223Z","shell.execute_reply.started":"2024-02-14T02:59:27.466115Z","shell.execute_reply":"2024-02-14T02:59:42.153128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IF THIS EQUALS NONE, THEN WE TRAIN NEW MODELS\n# IF THIS EQUALS DISK PATH, THEN WE LOAD PREVIOUSLY TRAINED MODELS\nLOAD_MODELS_FROM = '/kaggle/input/effnetb0-2-pop-model-train-twice-v1'\n\nUSE_KAGGLE_SPECTROGRAMS = True\nUSE_EEG_SPECTROGRAMS = True\n# USE MIXED PRECISION\nMIXED_PRECISION = True\n# READ ALL SPECTROGRAMS\nREAD_SPEC_FILES = False\n# # READ ALL EEG SPECTROGRAMS\nREAD_EEG_SPEC_FILES = False\n\nEVAL_ONLY = True","metadata":{"execution":{"iopub.status.busy":"2024-02-14T02:59:42.155393Z","iopub.execute_input":"2024-02-14T02:59:42.155882Z","iopub.status.idle":"2024-02-14T02:59:42.161181Z","shell.execute_reply.started":"2024-02-14T02:59:42.155856Z","shell.execute_reply":"2024-02-14T02:59:42.160089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\nVERSION = 1\n\nMODEL_NAME = f'effnetB0_2_pop_twice_train_v{VERSION}'\nMODEL_LOC = f'models/{MODEL_NAME}'\n\nNOTES = \"\"\"Split training data into 2 populations based on total vote sum.\nEffNetB0 efficientnet package with unique eeg_id and votes.\nUsing GKF and no augmentation.\n\"\"\"\n\nSEED = 2444\nBATCH_SIZE = 32\nFOLDS = 5 # 5-fold cross-validation.\n\n\nLR_START = 1e-4\nLR_MAX = 1e-4\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 2\nLR_STEP_DECAY = 0.5 # 0.5\nLR_EVERY = 1\nEPOCHS = 5\nPATIENCE = 2\nSTART_FROM_EPOCH = 2\n\nTIMESTAMP = pd.Timestamp.now('utc')\n\nmodel_info = {\n    'api': f'TensorFlow version = {tf.__version__}',\n    'datetime': TIMESTAMP.isoformat(),\n    'filename': 'efficientnet_tf_unique_vote.ipynb',\n    'folds': FOLDS,\n    'model': MODEL_NAME,\n    'notes': NOTES,\n    'path': MODEL_LOC,\n    'version': VERSION,\n    'SEED': SEED,\n    'BATCH_SIZE': BATCH_SIZE,\n    'EPOCHS': EPOCHS,\n    'FOLDS': FOLDS,\n    'PATIENCE': PATIENCE,\n    'USE_KAGGLE_SPECTROGRAMS': USE_KAGGLE_SPECTROGRAMS,\n    'USE_EEG_SPECTROGRAMS': USE_EEG_SPECTROGRAMS,\n    'MIXED_PRECISION': MIXED_PRECISION,\n    'READ_SPEC_FILES': READ_SPEC_FILES,\n    'READ_EEG_SPEC_FILES': READ_EEG_SPEC_FILES,\n    'LR_START': LR_START,\n    'LR_MAX': LR_MAX,\n    'LR_RAMPUP_EPOCHS': LR_RAMPUP_EPOCHS,\n    'LR_SUSTAIN_EPOCHS': LR_SUSTAIN_EPOCHS,\n    'LR_STEP_DECAY': LR_STEP_DECAY,\n    'LR_EVERY': LR_EVERY,\n    'START_FROM_EPOCH': START_FROM_EPOCH,\n     }\n\ntf.random.set_seed(\n    SEED\n)\nnp.random.seed(SEED)\n\n# Check if the directory exists\nif not os.path.exists(MODEL_LOC):\n    Path(MODEL_LOC).mkdir(parents=True, exist_ok=True)\n    \nmodel_info_path = os.path.join(MODEL_LOC, 'model_info.json')\nwith open(model_info_path, 'w') as f:\n    json.dump(model_info, f)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T02:59:42.164284Z","iopub.execute_input":"2024-02-14T02:59:42.164652Z","iopub.status.idle":"2024-02-14T02:59:42.194889Z","shell.execute_reply.started":"2024-02-14T02:59:42.164617Z","shell.execute_reply":"2024-02-14T02:59:42.194099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MIXED PRECISION\nif MIXED_PRECISION:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')","metadata":{"papermill":{"duration":0.016556,"end_time":"2024-01-14T22:51:51.671783","exception":false,"start_time":"2024-01-14T22:51:51.655227","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T02:59:42.195829Z","iopub.execute_input":"2024-02-14T02:59:42.196125Z","iopub.status.idle":"2024-02-14T02:59:42.20829Z","shell.execute_reply.started":"2024-02-14T02:59:42.196088Z","shell.execute_reply":"2024-02-14T02:59:42.207417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train Data","metadata":{"papermill":{"duration":0.007846,"end_time":"2024-01-14T22:51:51.688268","exception":false,"start_time":"2024-01-14T22:51:51.680422","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\nTARGETS = df.columns[-6:]\nprint('Train shape:', df.shape )\nprint('Targets', list(TARGETS))\n\ndf['total_evaluators'] = df[['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']].sum(axis=1)\n\ndf_uniq = df.drop_duplicates(subset=['eeg_id'] + list(TARGETS))\n# df_uniq = df_uniq.sort_values('eeg_id', ascending=True)\nprint(f'There are {df.patient_id.nunique()} patients in the training data.')\nprint(f'There are {df.eeg_id.nunique()} EEG IDs in the training data.')\nprint(f'There are {df_uniq.shape[0]} unique eeg_id + votes in the training data.')\n\ndf_uniq.eeg_id.value_counts().value_counts().plot(kind='bar', title=f'Distribution of Count of EEG w Unique Vote: '\n                                                                    f'{df_uniq.shape[0]} examples');","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.288611,"end_time":"2024-01-14T22:51:51.984993","exception":false,"start_time":"2024-01-14T22:51:51.696382","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T02:59:42.209451Z","iopub.execute_input":"2024-02-14T02:59:42.209721Z","iopub.status.idle":"2024-02-14T02:59:43.198212Z","shell.execute_reply.started":"2024-02-14T02:59:42.209697Z","shell.execute_reply":"2024-02-14T02:59:43.197237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif not EVAL_ONLY:\n    spectrograms = np.load('/kaggle/input/brain-spectrograms/specs.npy',allow_pickle=True).item()","metadata":{"papermill":{"duration":55.16894,"end_time":"2024-01-14T22:52:47.320438","exception":false,"start_time":"2024-01-14T22:51:52.151498","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T02:59:43.5031Z","iopub.execute_input":"2024-02-14T02:59:43.503525Z","iopub.status.idle":"2024-02-14T03:01:46.46647Z","shell.execute_reply.started":"2024-02-14T02:59:43.50349Z","shell.execute_reply":"2024-02-14T03:01:46.465248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif not EVAL_ONLY:\n    all_eegs = np.load('/kaggle/input/eeg-spectrogram-by-lead-id-unique/eeg_specs.npy',allow_pickle=True).item()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:01:46.467988Z","iopub.execute_input":"2024-02-14T03:01:46.468506Z","iopub.status.idle":"2024-02-14T03:04:18.753248Z","shell.execute_reply.started":"2024-02-14T03:01:46.468466Z","shell.execute_reply":"2024-02-14T03:04:18.752233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training DataFrame","metadata":{}},{"cell_type":"code","source":"if not EVAL_ONLY:\n    train = df[df['label_id'].isin(all_eegs.keys())].copy()\n    \n    pop_1_idx = train['total_evaluators'] < 10\n    \n    y_data = train[TARGETS].values\n    y_data = y_data / y_data.sum(axis=1,keepdims=True)\n    train[TARGETS] = y_data\n\n    train['target'] = train['expert_consensus']\n    \n    train_pop_1 = train[pop_1_idx].copy().reset_index()\n    train_pop_2 = train[~pop_1_idx].copy().reset_index()\n    # train = train.reset_index()\n    print('Pop 1: train unique eeg_id + votes shape:', train_pop_1.shape )\n    plt.figure(figsize=(10, 6))\n    plt.hist(train['total_evaluators'], bins=10, color='blue', edgecolor='black')\n    plt.title('Histogram of Total Evaluators')\n    plt.xlabel('Total Evaluators')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:04:18.754638Z","iopub.execute_input":"2024-02-14T03:04:18.755041Z","iopub.status.idle":"2024-02-14T03:04:19.099573Z","shell.execute_reply.started":"2024-02-14T03:04:18.755Z","shell.execute_reply":"2024-02-14T03:04:19.098598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\nTARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\nTARS2 = {x:y for y,x in TARS.items()}\n # 256\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, specs, eeg_specs,\n                 batch_size=32, shuffle=False, augment=False, mode='train'): \n        self.dim_1 = 256\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.mode = mode\n        self.specs = specs\n        self.eeg_specs = eeg_specs\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int( np.ceil( len(self.data) / self.batch_size ) )\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        if self.augment: X = self.__augment_batch(X) \n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.data) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n                        \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        \n        X = np.zeros((len(indexes),128,self.dim_1,8),dtype='float32')\n        y = np.zeros((len(indexes),6),dtype='float32')\n        img = np.ones((128,self.dim_1),dtype='float32')\n        \n        for j,i in enumerate(indexes):\n            row = self.data.iloc[i]\n            if self.mode=='test': \n                r = 0\n                spec_id = row.spec_id\n            elif self.mode in ['ensemble', 'granular_train', 'granular_valid']:\n                r = int(row['spectrogram_label_offset_seconds'] // 2)\n                spec_id = row['spectrogram_id']\n            else: \n                r = int( (row['min'] + row['max'])//4 )\n                spec_id = row.spec_id\n\n            for k in range(4):\n                # EXTRACT 300 ROWS OF SPECTROGRAM\n                img = self.specs[spec_id][r:r+300,k*100:(k+1)*100].T\n                \n                # LOG TRANSFORM SPECTROGRAM\n                img = np.clip(img,np.exp(-4),np.exp(8))\n                img = np.log(img)\n                \n                # STANDARDIZE PER IMAGE\n                ep = 1e-6\n                m = np.nanmean(img.flatten())\n                s = np.nanstd(img.flatten())\n                img = (img-m)/(s+ep)\n                img = np.nan_to_num(img, nan=0.0)\n                \n                # CROP TO 256 TIME STEPS\n                X[j,14:-14,:,k] = img[:,22:-22] / 2.0\n            \n            if self.mode in ['ensemble', 'granular_train', 'granular_valid']:\n                # ensemble uses label_id as a unique identifier\n                img = self.eeg_specs[row.label_id]\n            else:\n                # EEG SPECTROGRAMS\n                img = self.eeg_specs[row.eeg_id]\n            X[j,:,:,4:] = img\n\n            if self.mode!='test':\n                y[j,] = row[TARGETS]\n            \n        return X,y\n    \n    def __random_transform(self, img):\n        composition = albu.Compose([\n            albu.HorizontalFlip(p=0.5),\n            #albu.CoarseDropout(max_holes=8,max_height=32,max_width=32,fill_value=0,p=0.5),\n        ])\n        return composition(image=img)['image']\n            \n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ] = self.__random_transform(img_batch[i, ])\n        return img_batch","metadata":{"papermill":{"duration":2.369789,"end_time":"2024-01-14T22:52:49.721728","exception":false,"start_time":"2024-01-14T22:52:47.351939","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T03:04:19.102898Z","iopub.execute_input":"2024-02-14T03:04:19.103216Z","iopub.status.idle":"2024-02-14T03:04:20.335869Z","shell.execute_reply.started":"2024-02-14T03:04:19.10317Z","shell.execute_reply":"2024-02-14T03:04:20.335032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Scheduler","metadata":{"papermill":{"duration":0.026741,"end_time":"2024-01-14T22:52:52.251841","exception":false,"start_time":"2024-01-14T22:52:52.2251","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//LR_EVERY)\n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, y, 'o-'); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Step Training Schedule',size=16); plt.show()\n\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.309296,"end_time":"2024-01-14T22:52:52.92271","exception":false,"start_time":"2024-01-14T22:52:52.613414","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T03:04:20.337324Z","iopub.execute_input":"2024-02-14T03:04:20.338093Z","iopub.status.idle":"2024-02-14T03:04:20.621978Z","shell.execute_reply.started":"2024-02-14T03:04:20.33805Z","shell.execute_reply":"2024-02-14T03:04:20.621085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn2(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//LR_EVERY)\n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn2(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, y, 'o-'); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Step Training Schedule',size=16); plt.show()\n\nLR2 = tf.keras.callbacks.LearningRateScheduler(lrfn2, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:04:20.623374Z","iopub.execute_input":"2024-02-14T03:04:20.624019Z","iopub.status.idle":"2024-02-14T03:04:20.844235Z","shell.execute_reply.started":"2024-02-14T03:04:20.62398Z","shell.execute_reply":"2024-02-14T03:04:20.843238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build EfficientNet Model","metadata":{"papermill":{"duration":0.027228,"end_time":"2024-01-14T22:52:52.97653","exception":false,"start_time":"2024-01-14T22:52:52.949302","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links=/kaggle/input/tf-efficientnet-whl-files /kaggle/input/tf-efficientnet-whl-files/efficientnet-1.1.1-py3-none-any.whl","metadata":{"_kg_hide-output":true,"papermill":{"duration":13.587235,"end_time":"2024-01-14T22:53:06.589596","exception":false,"start_time":"2024-01-14T22:52:53.002361","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T03:04:20.845395Z","iopub.execute_input":"2024-02-14T03:04:20.845722Z","iopub.status.idle":"2024-02-14T03:04:34.105613Z","shell.execute_reply.started":"2024-02-14T03:04:20.845696Z","shell.execute_reply":"2024-02-14T03:04:34.104479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import efficientnet.tfkeras as efn\n\n\ndef build_model():\n    \n    inp = tf.keras.Input(shape=(128,300,10))\n    base_model = efn.EfficientNetB0(include_top=False, weights=None, input_shape=None)\n    base_model.load_weights('/kaggle/input/tf-efficientnet-noisy-student-weights/efficientnet-b0_noisy-student_notop.h5')\n    \n    # RESHAPE INPUT 128x256x8 => 512x512x3 MONOTONE IMAGE\n    # KAGGLE SPECTROGRAMS\n    x1 = [inp[:,:,:,i:i+1] for i in range(4)] #300\n    x1 = tf.keras.layers.Concatenate(axis=1)(x1)\n    # EEG SPECTROGRAMS\n    x2 = [inp[:,:,:,i+4:i+5] for i in range(4)]\n    x2 = tf.keras.layers.Concatenate(axis=1)(x2)\n    # MAKE 512X512X3\n    if USE_KAGGLE_SPECTROGRAMS & USE_EEG_SPECTROGRAMS:\n        x = tf.keras.layers.Concatenate(axis=2)([x1,x2])\n    elif USE_EEG_SPECTROGRAMS: \n        x = x2\n    else: \n        x = x1\n    # possible to change input channel?\n    x = tf.keras.layers.Concatenate(axis=3)([x,x,x])\n\n    # OUTPUT\n    x = base_model(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n#   x = tf.keras.layers.Dense(1024, activation='relu')(x)\n#   x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n\n    # Add your custom layers\n        \n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n#     opt = tf.keras.optimizers.legacy.Adam(learning_rate = 1e-3)\n    loss = tf.keras.losses.KLDivergence()\n\n    model.compile(loss=loss, optimizer = opt) \n        \n    return model","metadata":{"papermill":{"duration":0.057714,"end_time":"2024-01-14T22:53:06.682056","exception":false,"start_time":"2024-01-14T22:53:06.624342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T03:04:34.107141Z","iopub.execute_input":"2024-02-14T03:04:34.107528Z","iopub.status.idle":"2024-02-14T03:04:34.143401Z","shell.execute_reply.started":"2024-02-14T03:04:34.107495Z","shell.execute_reply":"2024-02-14T03:04:34.142616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nWe train using Group KFold on patient id. If `LOAD_MODELS_FROM = None`, then we will train new models in this notebook version. Otherwise we will load saved models from the path `LOAD_MODELS_FROM`.","metadata":{"papermill":{"duration":0.033717,"end_time":"2024-01-14T22:53:06.742557","exception":false,"start_time":"2024-01-14T22:53:06.70884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import io\nimport itertools\nimport matplotlib as mpl\nfrom tensorflow import keras\nfrom sklearn import metrics\n\ndef plot_confusion_matrix(cm, class_names):\n    figure = plt.figure(figsize=(8, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=mpl.colormaps['Greens'])\n    plt.title(\"Confusion matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n    threshold = cm.max() / 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"white\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    return figure\n\ndef plot_to_image(figure):\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png')\n    plt.close(figure)\n    buf.seek(0)\n\n    digit = tf.image.decode_png(buf.getvalue(), channels=4)\n    digit = tf.expand_dims(digit, 0)\n\n    return digit","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:04:34.144644Z","iopub.execute_input":"2024-02-14T03:04:34.145002Z","iopub.status.idle":"2024-02-14T03:04:34.156817Z","shell.execute_reply.started":"2024-02-14T03:04:34.144968Z","shell.execute_reply":"2024-02-14T03:04:34.15602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:04:34.157809Z","iopub.execute_input":"2024-02-14T03:04:34.158108Z","iopub.status.idle":"2024-02-14T03:04:34.172605Z","shell.execute_reply.started":"2024-02-14T03:04:34.158084Z","shell.execute_reply":"2024-02-14T03:04:34.171726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    import sys\n    sys.path.append('/kaggle/input/kaggle-kl-div')\n    from kaggle_kl_div import score\n\n    EARLY_STOP = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        restore_best_weights=True,\n        patience=PATIENCE,\n        start_from_epoch=START_FROM_EPOCH,\n        min_delta=0.0025\n    )\n\n    all_oof = []\n    all_true = []\n    all_histories = []\n \n    gkf = GroupKFold(n_splits=FOLDS)\n    val_indices = {}\n    val_label_ids = {}\n    preds={}\n    for fold_idx, (train_index, valid_index) in enumerate(gkf.split(train_pop_1, train_pop_1.target, train_pop_1.patient_id)):  \n        val_indices[fold_idx] = [int(i) for i in valid_index]\n    #     continue\n        print('#'*25)\n        print(f'### Fold {fold_idx+1}')\n        train_valid = train_pop_1.iloc[valid_index]\n        val_label_ids[fold_idx] = [int(i) for i in train_valid['label_id']]\n\n        train_gen = DataGenerator(train_pop_1.iloc[train_index], specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=True, batch_size=BATCH_SIZE, augment=False, mode='granular_train')\n        valid_gen = DataGenerator(train_valid, specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=False, batch_size=64, mode='granular_valid')\n\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n\n        K.clear_session()\n        with strategy.scope():\n            model = build_model()\n        h5_filename = f'EffNet_pop1_v{VERSION}_f{fold_idx}.h5'\n        if LOAD_MODELS_FROM is None:\n#             log_dir = os.path.join(\n#                 'logs/fit',\n#                 f'{TIMESTAMP.strftime(\"%Y%m%d-%H%M%S\")}_{MODEL_NAME}_k{fold_idx}_pop1')\n#             file_writer_cm = tf.summary.create_file_writer(os.path.join(log_dir, 'cm'))\n#             TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(\n#                 log_dir=log_dir,\n#                 histogram_freq=1,\n#                 write_graph=True,\n#                 write_images=False,\n#                 update_freq='epoch',\n#                 profile_batch=2,\n#                 embeddings_freq=1\n#             )\n            \n#             def log_confusion_matrix(epoch, logs):\n#                 predictions = model.predict(valid_gen, verbose=1)\n#                 predictions = np.argmax(predictions, axis=1)\n#                 true = np.argmax(train_valid[TARGETS].values, axis=1)\n\n#                 cm = metrics.confusion_matrix(true, predictions)\n#                 figure = plot_confusion_matrix(cm, class_names=TARGETS)\n#                 cm_image = plot_to_image(figure)\n\n#                 with file_writer_cm.as_default():\n#                     tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n\n#             CM = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n            # ModelCheckpoint callback to save the best model\n\n            h5_path = os.path.join(MODEL_LOC, h5_filename)\n            CHECKPOINT = tf.keras.callbacks.ModelCheckpoint(\n                h5_path,         # Path where to save the model\n                save_best_only=True,     # Only save a model if `val_loss` has improved\n                monitor='val_loss',      # Monitor 'val_loss' for improvement\n                mode='min'               # The smaller the `val_loss`, the better\n            )\n\n\n            history = model.fit(train_gen, verbose=1,\n                  validation_data = valid_gen,\n                  epochs=EPOCHS, callbacks = [EARLY_STOP, LR, CHECKPOINT, \n#                                               CM, TENSORBOARD_CALLBACK\n                                             ])\n            all_histories.append(history)\n            model.load_weights(h5_path)\n\n        else:\n            # load weights from pop 2 training to get oof \n            h5_filename = f'EffNet_pop2_v{VERSION}_f{fold_idx}.h5'\n            model.load_weights(os.path.join(LOAD_MODELS_FROM, h5_filename))\n\n        oof = model.predict(valid_gen, verbose=1)\n        all_oof.append(oof)\n        all_true.append(train_valid[TARGETS].values)\n\n        preds[fold_idx] = oof\n\n        del model, oof\n        gc.collect()\n\n\n    all_oof = np.concatenate(all_oof)\n    all_true = np.concatenate(all_true)\n\n\n    if LOAD_MODELS_FROM is None:\n        history_dict = {}\n        for fold, h in enumerate(all_histories):\n            history_dict[fold] = str(h.history)\n\n        with open('histories.json', 'w') as file:\n            json.dump(history_dict, file, indent=4)\n\n        with open('val_indices.json', 'w') as file:\n            json.dump(val_indices, file, indent=4)\n\n        with open('val_label_ids.json', 'w') as file:\n            json.dump(val_label_ids, file, indent=4)\n\n     \n    flattened_list = [item for sublist in val_label_ids.values() for item in sublist]\n    sub = pd.DataFrame({'label_id':flattened_list})\n    sub[TARGETS] = np.vstack(preds.values())\n    sub.to_csv(os.path.join(MODEL_LOC, 'ensemble_data1.csv'), index=False)\n    \n    oof = pd.DataFrame(all_oof.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    model_info['CV Score KL-Div Pop 1'] = cv\n    with open(model_info_path, 'w') as f:\n        json.dump(model_info, f)","metadata":{"papermill":{"duration":161.172,"end_time":"2024-01-14T22:55:47.94776","exception":false,"start_time":"2024-01-14T22:53:06.77576","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T03:31:31.414666Z","iopub.execute_input":"2024-02-14T03:31:31.415052Z","iopub.status.idle":"2024-02-14T03:34:04.360217Z","shell.execute_reply.started":"2024-02-14T03:31:31.415023Z","shell.execute_reply":"2024-02-14T03:34:04.359218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    import sys\n    sys.path.append('/kaggle/input/kaggle-kl-div')\n    from kaggle_kl_div import score\n    \n    EARLY_STOP = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        restore_best_weights=True,\n        patience=PATIENCE,\n        start_from_epoch=START_FROM_EPOCH,\n        min_delta=0.0025\n    )\n\n    all_oof2 = []\n    all_true2 = []\n    all_histories2 = []\n \n    gkf = GroupKFold(n_splits=FOLDS)\n    val_indices2 = {}\n    val_label_ids2 = {}\n    preds2={}\n    for fold_idx, (train_index, valid_index) in enumerate(gkf.split(train_pop_2, train_pop_2.target, train_pop_2.patient_id)):  \n\n        val_indices2[fold_idx] = [int(i) for i in valid_index]\n        print('#'*25)\n        print(f'### Fold {fold_idx+1}')\n        train_valid = train_pop_2.iloc[valid_index]\n        val_label_ids2[fold_idx] = [int(i) for i in train_valid['label_id']]\n\n        train_gen = DataGenerator(train_pop_2.iloc[train_index], specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=True, batch_size=BATCH_SIZE, augment=False, mode='granular_train')\n        valid_gen = DataGenerator(train_valid, specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=False, batch_size=64, mode='granular_valid')\n\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n\n        K.clear_session()\n        with strategy.scope():\n            model = build_model()\n        h5_filename_1 = f'EffNet_pop1_v{VERSION}_f{fold_idx}.h5'\n#         h5_filename_1 = f'EffNet_pop1_v1_f{fold_idx}.h5'\n        h5_filename_2 = f'EffNet_pop2_v{VERSION}_f{fold_idx}.h5'\n        if LOAD_MODELS_FROM is None:\n#             log_dir = os.path.join(\n#                 'logs/fit',\n#                 f'{TIMESTAMP.strftime(\"%Y%m%d-%H%M%S\")}_{MODEL_NAME}_k{fold_idx}_pop2')\n#             file_writer_cm = tf.summary.create_file_writer(os.path.join(log_dir, 'cm'))\n#             TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(\n#                 log_dir=log_dir,\n#                 histogram_freq=1,\n#                 write_graph=True,\n#                 write_images=False,\n#                 update_freq='epoch',\n#                 profile_batch=2,\n#                 embeddings_freq=1\n#             )\n            \n#             def log_confusion_matrix(epoch, logs):\n#                 predictions = model.predict(valid_gen, verbose=1)\n#                 predictions = np.argmax(predictions, axis=1)\n#                 true = np.argmax(train_valid[TARGETS].values, axis=1)\n\n#                 cm = metrics.confusion_matrix(true, predictions)\n#                 figure = plot_confusion_matrix(cm, class_names=TARGETS)\n#                 cm_image = plot_to_image(figure)\n\n#                 with file_writer_cm.as_default():\n#                     tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n\n#             CM = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n#             # ModelCheckpoint callback to save the best model\n\n            h5_path = os.path.join(MODEL_LOC, h5_filename_2)\n            CHECKPOINT = tf.keras.callbacks.ModelCheckpoint(\n                h5_path,         # Path where to save the model\n                save_best_only=True,     # Only save a model if `val_loss` has improved\n                monitor='val_loss',      # Monitor 'val_loss' for improvement\n                mode='min'               # The smaller the `val_loss`, the better\n            )\n\n            model.load_weights(os.path.join(MODEL_LOC, h5_filename_1))\n#             model.load_weights(os.path.join('/kaggle/input/temp-for-correction', h5_filename_1))\n            history = model.fit(train_gen, verbose=1,\n                  validation_data = valid_gen,\n                  epochs=EPOCHS, callbacks = [EARLY_STOP, LR, CHECKPOINT, \n#                                               CM, TENSORBOARD_CALLBACK\n                                             ])\n            all_histories2.append(history)\n            model.load_weights(h5_path)\n\n        else:\n            model.load_weights(os.path.join(LOAD_MODELS_FROM, h5_filename))\n\n        oof = model.predict(valid_gen, verbose=1)\n        all_oof2.append(oof)\n        all_true2.append(train_valid[TARGETS].values)\n\n        preds2[fold_idx] = oof\n\n        del model, oof\n        gc.collect()\n\n\n    all_oof2 = np.concatenate(all_oof2)\n    all_true2 = np.concatenate(all_true2)\n\n\n    if LOAD_MODELS_FROM is None:\n        history_dict = {}\n        for fold, h in enumerate(all_histories):\n            history_dict[fold] = str(h.history)\n\n        with open('histories.json', 'w') as file:\n            json.dump(history_dict, file, indent=4)\n\n        with open('val_indices.json', 'w') as file:\n            json.dump(val_indices, file, indent=4)\n\n        with open('val_label_ids.json', 'w') as file:\n            json.dump(val_label_ids, file, indent=4)\n\n     \n    flattened_list = [item for sublist in val_label_ids2.values() for item in sublist]\n    sub = pd.DataFrame({'label_id':flattened_list})\n    sub[TARGETS] = np.vstack(preds2.values())\n    sub.to_csv(os.path.join(MODEL_LOC, 'ensemble_data2.csv'), index=False)\n    \n    oof = pd.DataFrame(all_oof2.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true2.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    model_info['CV Score KL-Div Pop 2'] = cv\n    with open(model_info_path, 'w') as f:\n        json.dump(model_info, f)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:12:48.842619Z","iopub.execute_input":"2024-02-14T03:12:48.843071Z","iopub.status.idle":"2024-02-14T03:14:31.262616Z","shell.execute_reply.started":"2024-02-14T03:12:48.843036Z","shell.execute_reply":"2024-02-14T03:14:31.261605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    oof = pd.DataFrame(np.concatenate([all_oof, all_oof2]).copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(np.concatenate([all_true, all_true2]).copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    \n    # ens = pd.read_csv(os.path.join(MODEL_LOC, 'ensemble_data1.csv'))\n    # ens2 = pd.read_csv(os.path.join(MODEL_LOC, 'ensemble_data2.csv'))\n    # pd.concat([ens, ens2], ignore_index=True).to_csv('ensemble_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T03:24:11.975163Z","iopub.execute_input":"2024-02-14T03:24:11.97558Z","iopub.status.idle":"2024-02-14T03:24:12.046338Z","shell.execute_reply.started":"2024-02-14T03:24:11.97555Z","shell.execute_reply":"2024-02-14T03:24:12.045412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test and Create Submission CSV\nBelow we use our 5 EfficientNet fold models to infer the test data and create a `submission.csv` file.","metadata":{"papermill":{"duration":0.050491,"end_time":"2024-01-14T22:55:48.321932","exception":false,"start_time":"2024-01-14T22:55:48.271441","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pywt, librosa\n\nUSE_WAVELET = None \n\nNAMES = ['LL','LP','RP','RR']\n\nFEATS = [['Fp1','F7','T3','T5','O1'],\n         ['Fp1','F3','C3','P3','O1'],\n         ['Fp2','F8','T4','T6','O2'],\n         ['Fp2','F4','C4','P4','O2']]\n\n# DENOISE FUNCTION\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise(x, wavelet='haar', level=1):    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    ret=pywt.waverec(coeff, wavelet, mode='per')\n    \n    return ret\n\ndef spectrogram_from_eeg(parquet_path, display=False, offset=None):\n    \n    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n    eeg = pd.read_parquet(parquet_path)\n#     print(eeg.shape)\n    if offset is None:\n        middle = (len(eeg)-10_000)//2\n        eeg = eeg.iloc[middle:middle+10_000]\n    else:\n        eeg = eeg.iloc[offset:offset+10_000]\n    \n    # VARIABLE TO HOLD SPECTROGRAM\n    img = np.zeros((128,256,4),dtype='float32')\n    \n    if display: plt.figure(figsize=(10,7))\n    signals = []\n    for k in range(4):\n        COLS = FEATS[k]\n        \n        for kk in range(4):\n        \n            # COMPUTE PAIR DIFFERENCES\n            x = eeg[COLS[kk]].values - eeg[COLS[kk+1]].values\n\n            # FILL NANS\n            m = np.nanmean(x)\n            if np.isnan(x).mean() < 1: \n                x = np.nan_to_num(x,nan=m)\n            else: x[:] = 0\n\n            # DENOISE\n            if USE_WAVELET:\n                x = denoise(x, wavelet=USE_WAVELET)\n            signals.append(x)\n\n            # RAW SPECTROGRAM\n            mel_spec = librosa.feature.melspectrogram(y=x, sr=200, hop_length=len(x)//256, \n                  n_fft=1024, n_mels=128, fmin=0, fmax=20, win_length=128)\n\n            # LOG TRANSFORM\n            width = (mel_spec.shape[1]//32)*32\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[:,:width]\n\n            # STANDARDIZE TO -1 TO 1\n            mel_spec_db = (mel_spec_db+40)/40 \n            img[:,:,k] += mel_spec_db\n                \n        # AVERAGE THE 4 MONTAGE DIFFERENCES\n        img[:,:,k] /= 4.0\n        \n        if display:\n            plt.subplot(2,2,k+1)\n            plt.imshow(img[:,:,k],aspect='auto',origin='lower')\n#             plt.title(f'EEG {eeg_id} - Spectrogram {NAMES[k]}')\n            \n    if display: \n        plt.show()\n        plt.figure(figsize=(10,5))\n        offset = 0\n        for k in range(4):\n            if k>0: offset -= signals[3-k].min()\n            plt.plot(range(10_000),signals[k]+offset,label=NAMES[3-k])\n            offset += signals[3-k].max()\n        plt.legend()\n#         plt.title(f'EEG {eeg_id} Signals')\n        plt.show()\n        print(); print('#'*25); print()\n        \n    return img","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-14T01:24:48.869682Z","iopub.execute_input":"2024-02-14T01:24:48.870135Z","iopub.status.idle":"2024-02-14T01:24:48.896173Z","shell.execute_reply.started":"2024-02-14T01:24:48.870099Z","shell.execute_reply":"2024-02-14T01:24:48.895275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"if not EVAL_ONLY:\n    del all_eegs, spectrograms; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:24:48.8972Z","iopub.execute_input":"2024-02-14T01:24:48.897549Z","iopub.status.idle":"2024-02-14T01:24:48.903672Z","shell.execute_reply.started":"2024-02-14T01:24:48.897506Z","shell.execute_reply":"2024-02-14T01:24:48.902744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\nprint('Test shape',test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:24:48.904849Z","iopub.execute_input":"2024-02-14T01:24:48.905472Z","iopub.status.idle":"2024-02-14T01:24:48.928521Z","shell.execute_reply.started":"2024-02-14T01:24:48.90544Z","shell.execute_reply":"2024-02-14T01:24:48.927675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL SPECTROGRAMS\nPATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/'\nfiles2 = os.listdir(PATH2)\nprint(f'There are {len(files2)} test spectrogram parquets')\n    \nspectrograms2 = {}\nfor i,f in enumerate(files2):\n    if i%100==0: print(i,', ',end='')\n    tmp = pd.read_parquet(f'{PATH2}{f}')\n    name = int(f.split('.')[0])\n    spectrograms2[name] = tmp.iloc[:,1:].values\n    \n# RENAME FOR DATALOADER\ntest = test.rename({'spectrogram_id':'spec_id'},axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:24:48.92972Z","iopub.execute_input":"2024-02-14T01:24:48.930425Z","iopub.status.idle":"2024-02-14T01:24:49.188852Z","shell.execute_reply.started":"2024-02-14T01:24:48.930399Z","shell.execute_reply":"2024-02-14T01:24:49.188003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL EEG SPECTROGRAMS\nPATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\nDISPLAY = 1\nEEG_IDS2 = test.eeg_id.unique()\nall_eegs2 = {}\n\nprint('Converting Test EEG to Spectrograms...'); print()\nfor i,eeg_id in enumerate(EEG_IDS2):\n        \n    # CREATE SPECTROGRAM FROM EEG PARQUET\n    img = spectrogram_from_eeg(f'{PATH2}{eeg_id}.parquet', i<DISPLAY)\n    all_eegs2[eeg_id] = img","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:24:49.190136Z","iopub.execute_input":"2024-02-14T01:24:49.190437Z","iopub.status.idle":"2024-02-14T01:25:00.640432Z","shell.execute_reply.started":"2024-02-14T01:24:49.19041Z","shell.execute_reply":"2024-02-14T01:25:00.639461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFER EFFICIENTNET ON TEST\npreds = []\nmodel = build_model()\ntest_gen = DataGenerator(test, spectrograms2, all_eegs2, shuffle=False, batch_size=64, mode='test')\n\nfor i in range(FOLDS):\n    print(f'Fold {i+1}')\n    if LOAD_MODELS_FROM:\n        model.load_weights(os.path.join(LOAD_MODELS_FROM, f'EffNet_pop2_v{VERSION}_f{i}.h5'))\n    else:\n        model.load_weights(os.path.join(MODEL_LOC, f'EffNet_pop2_v{VERSION}_f{i}.h5'))\n    pred = model.predict(test_gen, verbose=1)\n    preds.append(pred)\npred = np.mean(preds,axis=0)\nprint()\nprint('Test preds shape',pred.shape)","metadata":{"papermill":{"duration":9.827745,"end_time":"2024-01-14T22:55:58.637732","exception":false,"start_time":"2024-01-14T22:55:48.809987","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T01:25:12.599018Z","iopub.execute_input":"2024-02-14T01:25:12.599402Z","iopub.status.idle":"2024-02-14T01:25:21.893894Z","shell.execute_reply.started":"2024-02-14T01:25:12.599375Z","shell.execute_reply":"2024-02-14T01:25:21.892822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\nsub[TARGETS] = pred\nsub.to_csv('submission.csv',index=False)\nprint('Submissionn shape',sub.shape)\nsub.head()","metadata":{"papermill":{"duration":0.071388,"end_time":"2024-01-14T22:55:58.760368","exception":false,"start_time":"2024-01-14T22:55:58.68898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T01:25:22.078281Z","iopub.execute_input":"2024-02-14T01:25:22.078604Z","iopub.status.idle":"2024-02-14T01:25:22.098723Z","shell.execute_reply.started":"2024-02-14T01:25:22.078579Z","shell.execute_reply":"2024-02-14T01:25:22.097783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\nsub.iloc[:,-6:].sum(axis=1)","metadata":{"papermill":{"duration":0.062742,"end_time":"2024-01-14T22:55:58.873394","exception":false,"start_time":"2024-01-14T22:55:58.810652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-14T01:25:22.7743Z","iopub.execute_input":"2024-02-14T01:25:22.774671Z","iopub.status.idle":"2024-02-14T01:25:22.784179Z","shell.execute_reply.started":"2024-02-14T01:25:22.774646Z","shell.execute_reply":"2024-02-14T01:25:22.783287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:25:23.413908Z","iopub.execute_input":"2024-02-14T01:25:23.414589Z","iopub.status.idle":"2024-02-14T01:25:24.418975Z","shell.execute_reply.started":"2024-02-14T01:25:23.41456Z","shell.execute_reply":"2024-02-14T01:25:24.417842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}